{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression Training Mean Square Error</h1> \n",
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will see what happens when you use the root mean square error cost or total loss function using random initialization for a parameter values.</p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Makeup_Data\">Make Some Data</a></li>\n",
    "    <li><a href=\"#Model_Cost\">Create the Model and Cost Function the PyTorch way</a></li>\n",
    "    <li><a href=\"#BGD\">Train the Model: Batch Gradient Descent</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>30 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for plotting and the function for plotting\n",
    "\n",
    "class plot_error_surfaces(object):\n",
    "    \n",
    "    # Construstor\n",
    "    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n",
    "        W = np.linspace(-w_range, w_range, n_samples)\n",
    "        B = np.linspace(-b_range, b_range, n_samples)\n",
    "        w, b = np.meshgrid(W, B)    \n",
    "        Z = np.zeros((30, 30))\n",
    "        count1 = 0\n",
    "        self.y = Y.numpy()\n",
    "        self.x = X.numpy()\n",
    "        for w1, b1 in zip(w, b):\n",
    "            count2 = 0\n",
    "            for w2, b2 in zip(w1, b1):\n",
    "                Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)\n",
    "                count2 += 1   \n",
    "            count1 += 1\n",
    "        self.Z = Z\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.LOSS = []\n",
    "        self.n = 0\n",
    "        if go == True:\n",
    "            plt.figure()\n",
    "            plt.figure(figsize = (7.5, 5))\n",
    "            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1, cmap = 'viridis', edgecolor = 'none')\n",
    "            plt.title('Loss Surface')\n",
    "            plt.xlabel('w')\n",
    "            plt.ylabel('b')\n",
    "            plt.show()\n",
    "            plt.figure()\n",
    "            plt.title('Loss Surface Contour')\n",
    "            plt.xlabel('w')\n",
    "            plt.ylabel('b')\n",
    "            plt.contour(self.w, self.b, self.Z)\n",
    "            plt.show()\n",
    "            \n",
    "     # Setter\n",
    "    def set_para_loss(self, model, loss):\n",
    "        self.n = self.n + 1\n",
    "        self.W.append(list(model.parameters())[0].item())\n",
    "        self.B.append(list(model.parameters())[1].item())\n",
    "        self.LOSS.append(loss)\n",
    "    \n",
    "    # Plot diagram\n",
    "    def final_plot(self): \n",
    "        ax = plt.axes(projection = '3d')\n",
    "        ax.plot_wireframe(self.w, self.b, self.Z)\n",
    "        ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n",
    "        plt.figure()\n",
    "        plt.contour(self.w, self.b, self.Z)\n",
    "        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('b')\n",
    "        plt.show()\n",
    "        \n",
    "    # Plot diagram\n",
    "    def plot_ps(self):\n",
    "        plt.subplot(121)\n",
    "        plt.ylim\n",
    "        plt.plot(self.x, self.y, 'ro', label = \"training points\")\n",
    "        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n",
    "        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label = 'sigmoid')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.ylim((-0.1, 2))\n",
    "        plt.title('Data Space Iteration: ' + str(self.n))\n",
    "        plt.show()\n",
    "        plt.subplot(122)\n",
    "        plt.contour(self.w, self.b, self.Z)\n",
    "        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n",
    "        plt.title('Loss Surface Contour Iteration' + str(self.n))\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('b')\n",
    "        \n",
    "# Plot the diagram\n",
    "\n",
    "def PlotStuff(X, Y, model, epoch, leg = True):\n",
    "    plt.plot(X.numpy(), model(X).detach().numpy(), label = 'epoch ' + str(epoch))\n",
    "    plt.plot(X.numpy(), Y.numpy(), 'r')\n",
    "    if leg == True:\n",
    "        plt.legend()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cfc0d9a3f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Get Some Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data class\n",
    "\n",
    "class Data(Dataset):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n",
    "        self.y = torch.zeros(self.x.shape[0], 1)\n",
    "        self.y[self.x[:, 0] > 0.2] = 1\n",
    "        self.len = self.x.shape[0]\n",
    "        \n",
    "    # Getter\n",
    "    def __getitem__(self, index):      \n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    # Get items\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data object\n",
    "data_set = Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Model_Cost\">Create the Model and Total Loss Function (Cost)</h2>\n",
    "\n",
    "Create a custom module for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logistic_regression(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(logistic_regression, self).__init__()\n",
    "        self.linear = nn.Linear(n_inputs, 1)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        yhat = torch.sigmoid(self.linear(x))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters:  OrderedDict([('linear.weight', tensor([[0.0957]])), ('linear.bias', tensor([0.9248]))])\n",
      "The parameters:  OrderedDict([('linear.weight', tensor([[-5.]])), ('linear.bias', tensor([-10.]))])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of logistic_regression and print its parameters\n",
    "\n",
    "model = logistic_regression(1)\n",
    "print(\"The parameters: \", model.state_dict())\n",
    "\n",
    "# Create plot_error_surfaces object\n",
    "#get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)\n",
    "\n",
    "# Set the weight and bias for bad initialization\n",
    "model.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\n",
    "model.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\n",
    "print(\"The parameters: \", model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader, cost function, optimizer\n",
    "trainloader = DataLoader(dataset=data_set, batch_size=3)\n",
    "criterion_rms = nn.MSELoss()\n",
    "learning_rate = 2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"BGD\">Train the Model via Batch Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy:  tensor(0.6500)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in trainloader:\n",
    "            yhat = model(x)\n",
    "            loss = criterion_rms(yhat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             get_surface.set_para_loss(model, loss.tolist())\n",
    "#         if epoch % 20 == 0:\n",
    "#             get_surface.plot_ps()\n",
    "\n",
    "train_model(100)\n",
    "\n",
    "\n",
    "# Get the actual class of each sample and calculate the accuracy on the test data.\n",
    "# Make prediction\n",
    "yhat = model(data_set.x)\n",
    "label = yhat > 0.5\n",
    "print(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "print((label == (data_set.y > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
